name: UNet2DNucleiBroad
description: A 2d U-Net pretrained on broad nucleus dataset.
cite:
    - text: "Ronneberger, Olaf et al. U-net: Convolutional networks for biomedical image segmentation. MICCAI 2015."
      doi: https://doi.org/10.1007/978-3-319-24574-4_28
authors:
  - Constantin Pape;@bioimage-io
  - Fynn Beuttenm√ºller
documentation: UNet2DNucleiBroad.md
tags: [unet2d, pytorch, nucleus-segmentation]
license: MIT

format_version: 0.1.0
language: python
framework: pytorch

source: pybio.torch.models.unet.UNet2d
optional_kwargs: {input_channels: 1, output_channels: 1}

test_input: null # ../test_input.npy
test_output: null # ../test_output.npy
covers: [] # ./nuclei_thumbnail.png

# TODO double check inputs/outputs
inputs:
  - name: raw
    axes: bcyx
    data_type: float32
    data_range: [-inf, inf]
    shape: [1, 1, 512, 512]
#        min: [1, 1, 32, 32]
#        step: [0, 0, 32, 32]
outputs:
  - name: output
    axes: bcyx
    data_type: float32
    data_range: [-inf, inf]
    halo: [0, 0, 32, 32]
    shape:
      reference_input: raw
      scale: [1, 1, 1, 1]
      offset: [0, 0, 0, 0]

prediction:
  preprocess:
    - spec: ../../../transformations/EnsureTorch.transformation.yaml
    - spec: ../../../transformations/Cast.transformation.yaml
      kwargs: {dtype: float32}
    - spec: ../../../transformations/NormalizeZeroMeanUnitVariance.transformation.yaml
      kwargs: {apply_to: [0]}
  weights:
      source: https://zenodo.org/record/3446812/files/unet2d_weights.torch
      hash: {md5: TODO}
  postprocess:
    - spec: ../../../transformations/Sigmoid.transformation.yaml
  dependencies: conda:../environment.yaml

training:
  setup:
#    meta_sampler:
#      spec:
#      kwargs:
    samplers:
      - spec: https://github.com/bioimage-io/python-bioimage-io/blob/7bb3fa932fd62b6d00539085e59f21908b4d8d55/specs/samplers/SequentialSamplerAlongDimension.sampler.yaml
        kwargs: {sample_dimensions: [0, 0]}
        readers:
          - spec: https://github.com/bioimage-io/python-bioimage-io/blob/7bb3fa932fd62b6d00539085e59f21908b4d8d55/specs/readers/BroadNucleusDataBinarized.reader.yaml
            transformations:
              - spec: ../../../transformations/NormalizeZeroMeanUnitVariance.transformation.yaml
                kwargs: {apply_to: [0]}
    preprocess: []
    postprocess:
      - spec: ../../../transformations/Sigmoid.transformation.yaml
        kwargs: {apply_to: [0]}
      - spec: ../../../transformations/Cast.transformation.yaml
        kwargs: {apply_to: [1], dtype: float32}
    losses:
      - spec: ../../../transformations/BCELoss.transformation.yaml
    optimizer:
      source: torch.optim.Adam
      required_kwargs: [params]
      optional_kwargs: {lr: 0.002}
    # validation:
    #   - {}

  source: pybio.torch.training.simple.simple_training
  required_kwargs: [pybio_model]
  optional_kwargs: {n_iterations: 25, batch_size: 4, num_workers: 2, out_file: ./weights.pytorch}
  # enable different ways of specifying the dependencies.
  # this would hold all training dependencies, e.g. as a frozen conda environment
  # or as a pom.xml
  dependencies: # this is a file to the dependencies
      conda:../environment.yaml
  description: Train the unet via binary cross entropy
